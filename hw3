---
title: "HW3 (140 points): DUE 4/15/2022"
author: "STAT 131A Spring 2022"
output:
  pdf_document
header-includes:
- \usepackage{framed}
- \usepackage{xcolor}
- \let\oldquote=\quote
- \let\endoldquote=\endquote
- \colorlet{shadecolor}{orange!15}
- \renewenvironment{quote}{\begin{shaded*}\begin{oldquote}}{\end{oldquote}\end{shaded*}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60), tidy=TRUE)
```

### Question 1 (5 points) 

Consider the question of comparing the difference between two groups. We decide to use the difference in the medians to evaluate whether there is any significant difference between the two groups. One of the histograms below is the histogram of difference in the medians from the permutations in the permutation test comparing these two groups. The other histogram is the histogram of the difference in the medians from bootstrap resampling of the data to create confidence intervals. Which is which? (explain your reasoning)

```{r bringInBlindData}
blindData<-read.table("blindData.txt",sep="\t",header=FALSE)
par(mfrow=c(1,2))
hist(blindData[,1],breaks=100,main="Plot A",xlab="")
hist(blindData[,2],breaks=100,main="Plot B",xlab="")
```

> The first graph is the permutation graph and the second one is the bootstrap graph. This is because bootstrap distributions will have the same mean as the observed value, since data is not being shuffled between both groups. This would lead to a difference in medians around 0, which is consistent with the second graph.


### Question 2 (10 points)

Assume you have a data.frame, `df` with a column `y` and a column `group`. `y` is the response variable, and `group` is a factor variable that gives which group the observation is in; there are two possible groups labeled `group1` and `group2` (i.e. levels of the factor). 

Consider the two following functions that 1) create a *single* bootstrap sample when given `df`  2) applies the function `FUN` to the bootstrap sample 3) returns the result of `FUN`. `FUN` can be any statistic that takes the data from two groups. 


```{r bootstrapTwoGroup}
bootFun1<-function(df, FUN){
    group1<-df$y[df$group=="group1"]
    group2<-df$y[df$group=="group2"]
    sample1 <- sample(x=group1, size=length(group1),replace = TRUE)
    sample2 <- sample(x=group2, size=length(group2),replace = TRUE)
    return(FUN(sample1,sample2))
}
bootFun2<-function(df, FUN){
    whObs <- sample(x=1:nrow(df), size=nrow(df),replace = TRUE)
    sampleDf<-df[whObs,]
    sample1<-sampleDf$y[sampleDf$group=="group1"]
    sample2<-sampleDf$y[sampleDf$group=="group2"]
    return(FUN(sample1,sample2))
}

test = data.frame(
  'group' = sample(c('group1', 'group2'), 10, replace = T),
  'y' = sample(1:10, 10, replace = T))

```

Describe the difference in the two strategies for re-sampling from the data.

> In bootFun1, sample1 is sampled from the list of rows that were in group1 (and same with sample2 and group2). Plus, since the samples are the length of each group, there will be exactly as many rows from each group in the bootstrap as there were in the original dataset. In bootFun2, it first takes a random sample of the whole dataset, then splits the sample into group1 and group2. The main difference between the two is that in bootFun1, sample1 is the same length as the number of group1 rows, whereas in bootFun2 the lengths of the sample and the groups differ. 

Hint: You will likely want to create a simple toy dataset to try this out and play around with the code in each one.

### Question 3

Consider the bootstrap function from the text (available on the website) for getting bootstrap confidence intervals for the results of fitting a line to the data using squared error.

```{r bootstrapLmFunction}
bootstrapLM <- function(y,x, repetitions, confidence.level=0.95){
  stat.obs <- coef(lm(y~x))
  bootFun<-function(){
	  sampled <- sample(1:length(y), size=length(y),replace = TRUE)
	  coef(lm(y[sampled]~x[sampled]))
  }  
  stat.boot<-replicate(repetitions,bootFun())
  # this next line that defines `nm` is advanced code, but 
  # simply finds the name of the input x value
  # and saves this name as `nm`.
  nm <-deparse(substitute(x))
  
  row.names(stat.boot)[2]<-nm
  level<-1-confidence.level
  confidence.interval <- apply(stat.boot,1,quantile,probs=c(level/2,1-level/2))  ### *** ###
  out<-cbind("lower"=confidence.interval[1,],"estimate"=stat.obs,"upper"=confidence.interval[2,])
  return(list(confidence.interval = out, bootStats=stat.boot))
}
```

a (5 points) Explain what the function `bootFun` does. Specifically, what does the two lines of code within `bootFun` do? And what type of output does `bootFun` return? (a character string? A number? A vector? A list? A matrix?)

> bootFun creates a bootstrap sample of x and y, then returns a list of the coefficients of their best fit line. Line 1 takes a sample of index values with replacement that's the same length as the data frame. Line 2 finds the best fit line between the x and y values of the sampled indexes, then returns the coefficients of the line as a list. The output is a named numeric vector.

b (5 points) Explain what type of object `stat.boot` is (a character string? A number? A vector? A list? A matrix?) and describe what are its actual entries 

> stat.boot is a matrix with the columns representing the bootstrap ID and the rows representing intercept/slope coefficients. The individual entries are doubles.  

c (5 points) Explain what the line  of the code marked with *** does (marked above). 

> It creates a matrix containing the 2.5 and 97.5 percentile (based on confidence.level) of all the intercept and slope coefficients. This is done by applying the quantile function over the rows of stat.boot. 


### Question 4

In the file `fitbit.csv` we have information from a fitbit device for a single person over the year. A fitbit device is something someone wears that records information regarding her activites throughout the day: her level of activity, the number of calories burned, the amount slept, etc (the company's website is www.fitbit.com). 

The data that we have here is summarize per day, so that each entry of the data corresponds to a particular date. 

The following code should read in the data and print out data from the first 5 rows of the first 8 columns.

```{r readInFitbit}
fitbit<-read.csv("fitbit.csv",header=TRUE)
head(fitbit[,1:8])
```

a (5 points) Many of the variables in this data set quantify the number of minutes spent doing activities, including sleeping. In principle, we might assume that these minutes should cover the whole day. However, the device might have been turned off or taken off at some point. 

Let's consider this question. Look at the following variables: `MinutesOfSleep`, `MinutesOfLightActivity`, `MinutesOfModerateActivity`, `MinutesOfSedentaryActivities`, and `MinutesOfIntenseActivity`. How well do these sum up to cover the entire day?

```{r totalMinutes}
# code to evaluate the total number of minutes
totMinutes = fitbit$MinutesOfIntenseActivity + fitbit$MinutesOfLightActivity + fitbit$MinutesOfModerateActivity + fitbit$MinutesOfSleep + fitbit$MinutesOfSedentaryActivities
summary(totMinutes)

```

> There are 1440 minutes in a day, and the quartiles shows that most data entries cover within 2-3 hours of the full day. However, the mean and minimum show that there is a significant left tail of days where the fitbit was barely active, and there are some values above 1440 minutes.

b (5 points) Going forward, we are going to consider the question of whether the amount of sedentary activity is predictive of the minutes of sleep per night. We are going to first normalize our data, so that both of these quantities are out of the total amount of minutes recorded in the day with the device. Calculate the total number of minutes recorded each day in the five variables from part a (call it `totMinutes`), and make new variables `pctAsleep` and `pctSedentary` that are the percentage of all tracked minutes. Print out a `summary` of each of these two variables to show that it worked.

```{r makePercentVariables}
#code to make new variables as percent of total minutes.
pctAsleep = fitbit$MinutesOfSleep / totMinutes
pctSedentary = fitbit$MinutesOfSedentaryActivities / totMinutes
summary(pctAsleep)
summary(pctSedentary)
```


c (10 points) Make a plot of `MinutesOfSleep` against `MinutesOfSedentaryActivites` and another plot of `pctAsleep` against `pctSedentary`. For the minutes plot, convert the minutes into hours. Use `par(mfrow= ...)` to make the two plots side-by-side (or something else like patchwork if you are using ggplot), and label the two plots appropriately. Comment on the affect of converting them to percentages.

```{r plotAsleepVsSedentary}
# code to plot the two plots side-by-side
par(mfrow = c(1, 2))
with(plot(MinutesOfSedentaryActivities/60, MinutesOfSleep/60, ylab = 'Hours of Sleep', xlab = 'Hours of Sedentary'), data = fitbit) #converts to hours
plot(pctSedentary, pctAsleep, ylab = 'Percentage of Time Asleep', xlab = 'Percentage of Time Sedentary')
```

> Converting to percentages helped normalize the data, which tightened the data set together and gave the data points extra context about the total amount of time recorded that day.

d (15 points) Plot the variable `pctAsleep` against `pctSedentary` like above, only now draw in the least squares regression line. Also print out the coefficients of the lines. Interpret the line and plots; be specific to this data -- e.g. as you increase percent of time spent in sedentary minutes by 10%, how is `pctAsleep` predicted to change?

```{r plotRegression}
# Code here for plot with regression line
lm.asleep.sedentary = lm(pctAsleep ~ pctSedentary)
plot(pctSedentary, pctAsleep)
abline(lm.asleep.sedentary)
print(coef(lm.asleep.sedentary))
```

> The line shows a shallow positive correlation which seems weakly correlated with the scatterplot. The linear regression shows that increasing pctSedentary by 10% should lead to pctAsleep increasing by 10*0.1492% = 1.492%.

e (15 points) We might consider the effect of the (practically) zero levels of sedentary activity. Remove these points from the above analysis in d, and compare the results to that found in d, and draw conclusions as to the effect of these data points. What conclusion would you draw as to whether they should be included in the analysis? 

```{r plotRegressionNoZero}
# Code here for plot with regression line removing zero sedentary
# I used 0.1 as the minimum threshold of sedentary activity
pctSedentaryAdj = pctSedentary[pctSedentary > 0.1]
pctAsleepAdj = pctAsleep[pctSedentary > 0.1]
lm.sa.adj = lm(pctAsleepAdj ~ pctSedentaryAdj)
plot(pctSedentaryAdj, pctAsleepAdj, ylim = c(0,1), xlab = 'Percentage Time Sedentary', ylab = 'Percentage Time Asleep')
abline(lm.sa.adj)
coef(lm.sa.adj)
```

> I believe they should be removed from the analysis, because we already know there are outlier data points where not enough time was recorded in the day from part a. Removing the 0 values of pctSedentary is akin to removing N/A values, as these data points do not represent a meaningful measurement. 

f (20 points) Create bootstrap confidence intervals as well as parametric confidence intervals for the coefficients of the regression you found in e. (you can use the function from Question 3). Plot the resulting confidence intervals and compare them.

```{r bootCI}
# Code here bootstrap and parametric confidence intervals
library(tidyverse)
bootstrapCI.sa = bootstrapLM(y = pctAsleepAdj, x = pctSedentaryAdj, 100)
parametricCI.sa = confint(lm.sa.adj)
# ggplot(aes(x = pctSedentaryAdj, y = pctAsleepAdj))
```

> The bootstrap intervals are larger than the parametric intervals, and neither of them included 0. Therefore, the null hypothesis that the slope between pctAsleep and pctSedentary is zero was rejected. 

g (5 points) We've seen that we do not always have all of the minutes of a day recorded. What affect could these missing minutes have on our above analysis? Be specific. A good idea is to first try to think of extreme examples of what could be happening in those minutes that might change your conclusion, so you can think about why not having that data might be a problem for your interpretation of the relationship between of sedentary activity and sleep levels. Then try to think of the best case scenario where lacking your data would not affect your results. Then return to the actual data (i.e. real life) and think how plausible it is to be worried about this problem. 

> The main issue is that missing minutes introduces discrepancies in both pctAsleep and pctSedentary. If the missing minutes were spent sleeping or being sedentary, the respective variable is deflated and the opposing variable inflated in the data set. If the time was spent doing neither, both variables are higher than the true value they represent. In the best case scenario, the time is not spent sleeping or being sedentary, in which case both variables should be inflated equally, preserving their relationship. However, since FitBits are most useful for tracking exercise, it is most likely that the unrecorded time was spent sleeping or not exercising, so it poses a significant problem to the interpretation of data. 


In the following questions, we will continue to work with the fitbit data, but we will modify the dataset. Below is code to convert the `Date` variable into a standard date class used by R, and then create variables that give the day of the week and the month of the date.

```{r readinFitbit}
fitbit<-read.csv("fitbit.csv",header=TRUE)
fitbit$Date<-as.Date(as.character(fitbit$Date),format="%d-%m-%Y")
fitbit$Day<-factor(weekdays(fitbit$Date),levels=weekdays(x=as.Date(seq(7), origin="1950-01-01")))
fitbit$Month<-factor(months(fitbit$Date),levels=month.name)
```

Notice how we explicitly give the function `factor` the levels to expect. This allows us to define what order they will be in, so we can force them to be in a proper order for days/months (to save on typing and possible typos, we have used built in functions in R to find the names of months and weeks in the right order, but you could have just typed them out too).

### Question 5  (5 points)

The code below changes the dataset to convert the minutes to percentages of the total, like the last problem, in addition to other changes. Describe what lines 3-5 do. You may need to look at the `help` of the function `gsub` and `abbreviate`. 

```{r modifyDataSet}
totalNumberOfMinutes <- fitbit$MinutesOfSleep + fitbit$MinutesOfLightActivity + fitbit$MinutesOfModerateActivity + fitbit$MinutesOfSedentaryActivities + fitbit$MinutesOfIntenseActivity #LINE 1
absoluteMinNames<-c("MinutesOfSedentaryActivities","MinutesOfLightActivity",
    "MinutesOfModerateActivity" , "MinutesOfIntenseActivity" , "activityCalories","MinutesOfSleep", 
    "MinutesOfBeingAwake" , "NumberOfAwakings","MinutesOfRest") #LINE 2
fitbit[,absoluteMinNames]<-fitbit[,absoluteMinNames]/totalNumberOfMinutes  #LINE 3
names(fitbit)<-gsub("Minutes","pctMin",names(fitbit)) #LINE 4
names(fitbit)<-abbreviate(names(fitbit),10) #LINE 5
```

> Line 3 converts each measurement of minutes in fitbit (MinutesOfSedentaryActivities, MinutesOfLightActivity, etc) into a proportion of the total time recorded that day. 
> Line 4 replaces the "Minutes" portion of the column names with "pctMin" so they better represent the modified values. 
> Line 5 abbreviates these column names to 10 characters using the abbreviate() algorithm. 

For the rest of the problems, use this modified data frame.

### Question 6 (10 points) 

Create a pairs plot of the continuous variables in the dataset, except for `distance` and `plans`, and color code by points by the day of the week.  We have defined the colors for you in the following chunk with the vector `colWeek`. 

```{r pairsPlot}
colWeek<-palette()[1:nlevels(fitbit$Day)]
names(colWeek)<-levels(fitbit$Day)
colWeek

#Code for pairs plot here.
pairs(fitbit[c(2, 3, 6:14)], col = colWeek)
```

Comment which variables appear to have a strong pairwise relationship with `calorsBrnd`. 

> steps is the only variable strongly related to calorsBrnd. 

### Question 7

(a) (5 points) Plot the percent of sedentary activity as a function of `Date`, making sure you have removed the (practically) zero values (as you did in 4e). Color the points according the day of the week and give a legend. 

```{r plotByDate}
# code to plot sedentary against date
# fitbit$Date is adjusted to only have values corresponding to adjusted sedentary values
plot(fitbit$Date[pctSedentary > 0.1], pctSedentaryAdj, col = colWeek)
legend('topleft', levels(fitbit$Day), fill = colWeek, cex = 0.7)
```

(b) (10 points) Draw loess lines for for each day of the week that fit the percent of sedentary activity as a function of date. The loess lines should all be on the same plot. Note that you will need to do `as.numeric` for the `fitbit$Date` object for loess to convert it from a date to a number for fitting the loess line. 

Color each line by the day of the week, as in the previous plot, and include a legend. Use either a for-loop or the `by` function to do this. 

The first line of your code should set up a blank plot. Do this by reusing your code from (a) above (i.e. that plots sedentary against the data), but include the argument `type="n"` in your plot command. This instructs R to set up the axes, etc. but not actually plot the points (it could also be interesting to draw the loess lines *with* the points as well, but for this assignment draw just the lines). 

```{r plotLoessPerDay}
# code to plot loess lines
fitbit.date.adj = fitbit[fitbit$pctMnOfSdA > 0.1, c('Date', 'Day', 'pctMnOfSdA')]
fitbit.date.adj$Date = as.numeric(fitbit.date.adj$Date)
with(fitbit.date.adj, plot(Date, pctMnOfSdA, type = 'n'))
for (i in levels(fitbit$Day)){
  dat = fitbit.date.adj[fitbit.date.adj$Day == i, c('Date', 'pctMnOfSdA')]
  fitbit.lw = with(dat, loess(pctMnOfSdA ~ Date))
  j = order(dat$Date)
  lines(dat$Date[j], fitbit.lw$fitted[j], col = colWeek[i],  lwd = 2)
}
legend('topleft', levels(fitbit$Day), fill = colWeek, cex = 0.7)
```

(c) (5 points) Comment on whether there are any differences due to the day of the week.

> Saturday consistently has the least sedentary percentage, whereas weekdays like Monday-Wednesday are generally more sedentary. 

